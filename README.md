# DinÃ¢mica de Focos de Queimadas no Brasil (2019â€“2024) â€” Projeto Aplicado I

**Grupo:**  

- ANA CLARA SILVA DE SOUZA
- CID WALLACE ARAUJO DE OLIVEIRA
- EDUARDO MACHADO SILVA
- FREDERICO RIPAMONTE BORGES

**RepositÃ³rio:** [github.com/fredericorbgs/projeto_aplicado_grupo_12](https://github.com/fredericorbgs/projeto_aplicado_grupo_12/)

## Objetivo do estudo

Desenvolver um estudo prÃ¡tico com dois produtos:

1) **AnÃ¡lise ExploratÃ³ria de Dados (AED)** dos focos de queimadas no Brasil 
(2019â€“2024).  
2) **Proposta analÃ­tica** aplicÃ¡vel ao conjunto de dados selecionado (detecÃ§Ã£o de anomalias e priorizaÃ§Ã£o territorial).

## Contexto organizacional (origem dos dados)

Os dados sÃ£o produzidos e mantidos pelo **INPE â€” Programa Queimadas** (organizaÃ§Ã£o pÃºblica federal), cujo objetivo Ã© monitorar e divulgar informaÃ§Ãµes sobre queimadas/incÃªndios florestais no territÃ³rio nacional e paÃ­ses vizinhos, apoiando polÃ­ticas pÃºblicas e a gestÃ£o ambiental.

## Problema de pesquisa (caracterizaÃ§Ã£o)

Como **caracterizar a dinÃ¢mica espaÃ§o-temporal** dos focos de calor no Brasil, identificando **sazonalidade por bioma/UF/municÃ­pio**, **anomalias** (picos atÃ­picos) e **Ã¡reas crÃ­ticas** para priorizaÃ§Ã£o de aÃ§Ãµes?

## Base de dados (metadados-chave)

- **Arquivos:** CSVs anuais (2019â€“2024) â€” diretÃ³rio: `/queimadas/focos/csv/anual/AMS_sat_ref/`  
- **Granularidade:** ponto (foco) com coordenadas geogrÃ¡ficas e carimbo de data/hora  
- **Colunas mÃ­nimas (exemplo):** `id_bdq, foco_id, lat, lon, data_pas, pais, estado, municipio, bioma`  
- **ObservaÃ§Ãµes tÃ©cnicas:** pode haver diferenÃ§as de encoding (ex.: `VITâˆšÃ’RIA DA CONQUISTA` â†’ *VitÃ³ria da Conquista*). Tratar com `encoding='latin1'` ou normalizaÃ§Ã£o UTF-8.

## Abordagem por etapas

- **Etapa 1 (30 dias, entrega 11/09/2025):** premissas, objetivos e metas, cronograma, definiÃ§Ã£o de grupos e mapeamento de **Pensamento Computacional** ao contexto do INPE.  
- **Etapa 2 (30 dias):** AED completa + **proposta analÃ­tica** formal (alvo, features, mÃ©trica, validaÃ§Ã£o).  
- **Etapa 3 (30 dias):** **Data Storytelling** (narrativa visual, painÃ©is e relatÃ³rio executivo).  
- **Etapa 4 (30 dias):** ajustes finais e apresentaÃ§Ã£o.

## Proposta analÃ­tica (rascunho para Etapa 2)

- **Alvo:** detectar **dias/municÃ­pios anÃ´malos** (picos de focos acima do esperado) por **bioma/UF**, usando tendÃªncia+sazonalidade (ex.: mÃ©dia mÃ³vel/ETS) e *z-score* robusto.  
- **SaÃ­das:** ranking de **Ã¡reas crÃ­ticas** com percentis e bandas de confianÃ§a; painel com *sparkline* por bioma/UF/municÃ­pio.  
- **MÃ©tricas:** precisÃ£o na identificaÃ§Ã£o de picos histÃ³ricos conhecidos; estabilidade das faixas (backtesting simples).

## Etapa 2 - ImplementaÃ§Ã£o Completa âœ…

### Estrutura do RepositÃ³rio

```
projeto_aplicado_grupo_12/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/queimadas/          # CSVs originais (2019-2024, nÃ£o versionados)
â”‚   â””â”€â”€ processed/              # Dados processados e artefatos CSV
â”‚       â”œâ”€â”€ focos_2019_2024.parquet  # Dataset consolidado (108 MB)
â”‚       â”œâ”€â”€ resumo_colunas.csv
â”‚       â”œâ”€â”€ estatisticas_gerais.csv
â”‚       â””â”€â”€ anomalias_top.csv
â”œâ”€â”€ figs/eda/                   # Figuras geradas pela anÃ¡lise
â”‚   â”œâ”€â”€ series_bioma.png
â”‚   â”œâ”€â”€ boxplot_bioma.png
â”‚   â””â”€â”€ top10_uf.png
â”œâ”€â”€ src/                        # Scripts Python
â”‚   â”œâ”€â”€ pipeline_ingestao.py   # ETL e consolidaÃ§Ã£o dos CSVs
â”‚   â””â”€â”€ eda_utils.py           # AnÃ¡lise exploratÃ³ria e visualizaÃ§Ãµes
â”œâ”€â”€ notebooks/                  # Notebooks Jupyter
â”‚   â”œâ”€â”€ EDA_inicial.ipynb
â”‚   â””â”€â”€ EDA_queimadas_etapa2.ipynb  # Notebook da Etapa 2
â”œâ”€â”€ docs/                       # DocumentaÃ§Ã£o LaTeX
â”‚   â”œâ”€â”€ projeto_aplicado.tex   # Documento principal
â”‚   â”œâ”€â”€ projeto_aplicado.pdf   # PDF compilado (15 pÃ¡ginas)
â”‚   â”œâ”€â”€ cronograma.md
â”‚   â””â”€â”€ etapa1_entrega.md
â””â”€â”€ requirements.txt            # DependÃªncias Python
```

### Como Executar

#### 1. Instalar DependÃªncias

```bash
pip install -r requirements.txt
```

#### 2. Executar Pipeline de IngestÃ£o

Consolida os CSVs anuais (2019-2024) em um Ãºnico arquivo Parquet:

```bash
python -m src.pipeline_ingestao
```

**SaÃ­da:**
- `data/processed/focos_2019_2024.parquet` (2.008.071 registros limpos)

#### 3. Executar AnÃ¡lise ExploratÃ³ria

Gera estatÃ­sticas, figuras e detecÃ§Ã£o de anomalias:

```bash
python -m src.eda_utils
```

**SaÃ­das:**
- `data/processed/resumo_colunas.csv`
- `data/processed/estatisticas_gerais.csv`
- `data/processed/anomalias_top.csv`
- `figs/eda/*.png` (3 figuras)

#### 4. Explorar com Jupyter Notebook

```bash
jupyter notebook notebooks/EDA_queimadas_etapa2.ipynb
```

#### 5. Compilar Documento LaTeX

```bash
cd docs/
pdflatex projeto_aplicado.tex
pdflatex projeto_aplicado.tex  # Segunda compilaÃ§Ã£o para referÃªncias
```

**SaÃ­da:** `docs/projeto_aplicado.pdf`

### Artefatos da Etapa 2

- ğŸ“Š **Dataset consolidado**: 2M+ registros de focos (2019-2024)
- ğŸ“ˆ **3 figuras principais**: sÃ©ries temporais, boxplots, rankings
- ğŸ“ **4 CSVs de anÃ¡lise**: resumos, estatÃ­sticas e anomalias
- ğŸ“„ **Documento PDF**: 15 pÃ¡ginas com proposta analÃ­tica e AED completa
- ğŸ’» **CÃ³digo Python**: scripts modulares e notebook reproduzÃ­vel

## Cronograma (Etapa 1)

Ver `docs/cronograma.md` para atividades, datas, responsÃ¡veis e milestones.

## Data Storytelling (visÃ£o â€” Etapa 3)

TrÃªs atos: **Onde e quando ocorre** (mapas e sÃ©ries) â†’ **QuÃ£o atÃ­pico Ã©** (anomalias) â†’ **O que priorizar** (ranking e recomendaÃ§Ãµes).
